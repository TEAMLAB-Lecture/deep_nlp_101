통계적 언어 모델링의 목표는 언어에서 일련의 단어의 결합 확률 함수를 학습하는 것입니다.

이는 차원의 저주로 인해 본질적으로 어렵습니다. 
모델을 테스트 할 단어 순서는 학습 중에 나타나는 모든 단어 순서와 다를 수 있습니다.

전통적이지만 매우 성공적이었던 n-gram에 기반한 접근법은 훈련 세트에서 보이는 매우 짧은 중복 시퀀스를 연결함으로써 일반화를 얻습니다. 
우리는 각 훈련 문장이 의미 론적으로 이웃하는 문장의 지수적인 수에 대해 모형을 알리는 것을 허용하는 단어에 대한 분산 표현을 학습함으로써 차원의 저주와 싸울 것을 제안한다. 
이 모델은 각 단어에 대한 분산 된 표현과 단어 구조에 대한 확률 함수를 동시에 표현합니다 (1). 일반화는 이미 본 적이없는 단어의 순서가 이미 본 문장을 형성하는 단어와 비슷한 단어 (가까운 표현을 가졌음을 의미)로 이루어지면 높은 확률을 갖기 때문에 얻어집니다. 합리적인 시간 내에 이러한 대규모 모델 (수백만 가지 매개 변수 포함)을 교육하는 것은 중요한 과제입니다. 확률 함수에 대한 신경망을 이용한 실험을 통해 제안 된 접근법이 최첨단 n-gram 모델을 크게 향상시키고 제안 된 접근법이보다 긴 맥락을 이용할 수 있음을 두 텍스트 코서에 보여 주었다.